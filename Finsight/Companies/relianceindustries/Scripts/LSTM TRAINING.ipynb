{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjT3ZmGoCrwt",
        "outputId": "bb16073a-5d93-42fb-d2b5-692c4737f258"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 10 Complete [00h 00m 38s]\n",
            "val_loss: 0.02987702563405037\n",
            "\n",
            "Best val_loss So Far: 0.009773152880370617\n",
            "Total elapsed time: 00h 04m 20s\n",
            "\n",
            "Tuner search complete. Retrieving best model...\n",
            "\n",
            "        Best model hyperparameters:\n",
            "        - Units: 128\n",
            "        - Dropout 1: 0.30\n",
            "        - Dropout 2: 0.10\n",
            "        - Learning Rate: 0.001\n",
            "        \n",
            "\n",
            "Retraining the best model on the full training data...\n",
            "Epoch 1/50\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - loss: 0.0443 - val_loss: 0.0728\n",
            "Epoch 2/50\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0014 - val_loss: 0.0354\n",
            "Epoch 3/50\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 8.3049e-04 - val_loss: 0.0223\n",
            "Epoch 4/50\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 6.6959e-04 - val_loss: 0.0195\n",
            "Epoch 5/50\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 5.1652e-04 - val_loss: 0.0235\n",
            "Epoch 6/50\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 4.2146e-04 - val_loss: 0.0254\n",
            "Epoch 7/50\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 3.8989e-04 - val_loss: 0.0169\n",
            "Epoch 8/50\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 3.5170e-04 - val_loss: 0.0182\n",
            "Epoch 9/50\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 3.0556e-04 - val_loss: 0.0186\n",
            "Epoch 10/50\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 2.3017e-04 - val_loss: 0.0167\n",
            "Epoch 11/50\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 2.5582e-04 - val_loss: 0.0125\n",
            "Epoch 12/50\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 3.0105e-04 - val_loss: 0.0148\n",
            "Epoch 13/50\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 2.3192e-04 - val_loss: 0.0161\n",
            "Epoch 14/50\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 2.2241e-04 - val_loss: 0.0187\n",
            "Epoch 15/50\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 1.7543e-04 - val_loss: 0.0117\n",
            "Epoch 16/50\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 1.6102e-04 - val_loss: 0.0111\n",
            "Epoch 17/50\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 1.9103e-04 - val_loss: 0.0112\n",
            "Epoch 18/50\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 2.0634e-04 - val_loss: 0.0097\n",
            "Epoch 19/50\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 1.6754e-04 - val_loss: 0.0143\n",
            "Epoch 20/50\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 1.6083e-04 - val_loss: 0.0143\n",
            "Epoch 21/50\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 1.5736e-04 - val_loss: 0.0117\n",
            "Epoch 22/50\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 1.5153e-04 - val_loss: 0.0198\n",
            "Epoch 23/50\n",
            "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 1.4638e-04 - val_loss: 0.0149\n",
            "\n",
            "Saving the tuned LSTM model to reliance_lstm_model_tuned.keras...\n",
            "Model saved successfully.\n",
            "\n",
            "--- LSTM Model Training Completed ---\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "import keras_tuner as kt\n",
        "import os\n",
        "\n",
        "X_train, y_train, X_val, y_val = None, None, None, None\n",
        "\n",
        "def create_dataset(X, y, time_steps=1):\n",
        "\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - time_steps):\n",
        "        v = X[i:(i + time_steps)] \n",
        "        Xs.append(v)\n",
        "        ys.append(y[i + time_steps]) \n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "def build_model(hp):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    hp_units = hp.Int('units', min_value=32, max_value=128, step=32)\n",
        "\n",
        "    model.add(LSTM(units=hp_units, return_sequences=True,\n",
        "                   input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "\n",
        "    model.add(Dropout(hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)))\n",
        "\n",
        "    model.add(LSTM(units=hp_units, return_sequences=False))\n",
        "\n",
        "    model.add(Dropout(hp.Float('dropout_2', min_value=0.1, max_value=0.5, step=0.1)))\n",
        "\n",
        "    model.add(Dense(units=25))\n",
        "    model.add(Dense(units=1))\n",
        "\n",
        "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                  loss='mean_squared_error')\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_lstm_model():\n",
        "\n",
        "    global X_train, y_train, X_val, y_val\n",
        "\n",
        "    master_dataset_path = 'reliance_master_dataset.csv' \n",
        "    output_model_path = 'reliance_lstm_model_tuned.keras' #\n",
        "    time_steps = 60 \n",
        "    print(\"\\nStarting LSTM Model Tuning and Training for Reliance Industries \")\n",
        "    try:\n",
        "        print(f\"Loading data from {master_dataset_path}...\")\n",
        "        if not os.path.exists(master_dataset_path):\n",
        "            raise FileNotFoundError(f\"Master dataset not found: {master_dataset_path}\")\n",
        "\n",
        "        df = pd.read_csv(master_dataset_path)\n",
        "        df['date'] = pd.to_datetime(df['date'])\n",
        "        df.set_index('date', inplace=True)\n",
        "\n",
        "        features = df.select_dtypes(include=np.number)\n",
        "        target = features['close'].values.reshape(-1, 1)\n",
        "\n",
        "        print(\"Scaling features...\")\n",
        "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "        scaled_features = scaler.fit_transform(features)\n",
        "\n",
        "        target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "        scaled_target = target_scaler.fit_transform(target)\n",
        "\n",
        "        print(f\"Creating sequences with {time_steps} time steps...\")\n",
        "        X, y = create_dataset(scaled_features, scaled_target, time_steps)\n",
        "\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
        "        print(f\"Training data shape: {X_train.shape}\")\n",
        "        print(f\"Validation data shape: {X_val.shape}\")\n",
        "\n",
        "        print(\"\\nSetting up KerasTuner for hyperparameter search...\")\n",
        "\n",
        "        tuner = kt.RandomSearch(\n",
        "            build_model,\n",
        "            objective='val_loss',\n",
        "            max_trials=10,\n",
        "            executions_per_trial=1,\n",
        "            directory='reliance_lstm_tuner', \n",
        "            project_name='reliance_price_prediction'\n",
        "        )\n",
        "\n",
        "        stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "        print(\"\\nStarting tuner search... (This will take a significant amount of time)\")\n",
        "\n",
        "        tuner.search(X_train, y_train, epochs=50, validation_data=(X_val, y_val), callbacks=[stop_early])\n",
        "\n",
        "        print(\"\\nTuner search complete. Retrieving best model...\")\n",
        "        best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "        print(f\"\"\"\n",
        "        Best model hyperparameters:\n",
        "        - Units: {best_hps.get('units')}\n",
        "        - Dropout 1: {best_hps.get('dropout_1'):.2f}\n",
        "        - Dropout 2: {best_hps.get('dropout_2'):.2f}\n",
        "        - Learning Rate: {best_hps.get('learning_rate')}\n",
        "        \"\"\")\n",
        "\n",
        "        model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "        print(\"\\nRetraining the best model on the full training data...\")\n",
        "        history = model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), callbacks=[stop_early])\n",
        "\n",
        "        print(f\"\\nSaving the tuned LSTM model to {output_model_path}...\")\n",
        "        model.save(output_model_path)\n",
        "        print(\"Model saved successfully.\")\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: {e}. Please ensure the master dataset was created successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during LSTM model training: {e}\")\n",
        "\n",
        "    print(\"\\n--- LSTM Model Training Completed ---\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_lstm_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkdT3dDHKcwy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "name": "Welcome To Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
